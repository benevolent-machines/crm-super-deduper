{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyM1qLIpCffXqefsUgTvRRAF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benevolent-machines/crm-super-deduper/blob/main/main-module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CRM Super Deduper Module\n",
        "\n",
        "https://github.com/benevolent-machines/crm-super-deduper\n"
      ],
      "metadata": {
        "id": "ACWPuMCsWng7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _load_data()"
      ],
      "metadata": {
        "id": "0xtY_qFDYhZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _load_data(input_data):\n",
        "    import pandas as pd\n",
        "\n",
        "    if isinstance(input_data, str):\n",
        "        file_extension = input_data.split('.')[-1].lower()\n",
        "        if file_extension == 'csv':\n",
        "            df = pd.read_csv(input_data)\n",
        "        elif file_extension in ['xlsx', 'xls']:\n",
        "            df = pd.read_excel(input_data)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"load_data(): Unsupported file type: {file_extension}\")\n",
        "    elif isinstance(input_data, pd.DataFrame):\n",
        "        df = input_data\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"load_data(): Unsupported file type: {file_extension}\")\n",
        "    return df\n",
        "#_load_data(\"test.csv\")"
      ],
      "metadata": {
        "id": "vS0K-IvSY6_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _tfidf_neighbors()"
      ],
      "metadata": {
        "id": "xWk433cYAoha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _tfidf_neighbors(\n",
        "        pandas_dataframe,\n",
        "        entity_id='entity_id',\n",
        "        columns=['name', 'address', 'city', 'state'],\n",
        "        threshold=0.7\n",
        "    ):\n",
        "\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "    df = pandas_dataframe\n",
        "    print(df.shape)\n",
        "\n",
        "    # descending by entity_id so we start with the latest records\n",
        "    df = df.sort_values(entity_id, ascending=False).reset_index(drop=True)\n",
        "\n",
        "    df[columns] = df[columns].astype(str) # clean up NaN\n",
        "    df['combined'] = df[columns].agg(' '.join, axis=1) # TF/IDF documents\n",
        "    df['combined'] = df['combined'].str.upper()\n",
        "    df['dup_id'] = df[entity_id]\n",
        "    df['dup_flag'] = 0\n",
        "    df['score'] = 0.0\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(df['combined'])\n",
        "    nbrs = NearestNeighbors(radius=threshold, metric='cosine').fit(tfidf_matrix)\n",
        "    distances, indices = nbrs.radius_neighbors(tfidf_matrix)\n",
        "\n",
        "    for i, (dist, idx) in enumerate(zip(distances, indices)):\n",
        "        for d, j in zip(dist, idx):\n",
        "            if i < j:  # only compare with what comes after:\n",
        "                #print('distance',d)\n",
        "                similarity_score = 1 - d  # Convert distance to similarity\n",
        "                if similarity_score > threshold:\n",
        "                    if df.loc[i, 'dup_flag'] == 0:\n",
        "                        df.loc[i, 'dup_flag'] = 2\n",
        "                    df.loc[j, 'dup_flag'] = 1\n",
        "                    if df.loc[j, 'dup_id'] == df.loc[j, entity_id]:\n",
        "                        df.loc[j, 'dup_id'] = df.loc[i, entity_id]\n",
        "                        if df.loc[i, 'score'] == 0.0:\n",
        "                            df.loc[i, 'score'] = 1.0\n",
        "                        df.loc[j, 'score'] = round(similarity_score,3)\n",
        "    return df"
      ],
      "metadata": {
        "id": "QsS1TfZfg4No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _tfidf_neighbors_subgrouped()"
      ],
      "metadata": {
        "id": "PYcf0ALXPtxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _tfidf_neighbors_subgrouped(\n",
        "        pandas_dataframe,\n",
        "        entity_id='entity_id',\n",
        "        columns=['name', 'address', 'city', 'state'],\n",
        "        subgroup_column='state',  # Specify the column to use for subgrouping\n",
        "        threshold=0.7\n",
        "    ):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "    df = pandas_dataframe\n",
        "\n",
        "    # Ensure the subgroup column is a string\n",
        "    df[subgroup_column] = df[subgroup_column].astype(str)\n",
        "\n",
        "    # Create a subgroup identifier based on the first 2 characters of the specified column\n",
        "    df['subgroup'] = df[subgroup_column].str[:2].str.upper()\n",
        "\n",
        "    # Initialize a list to collect dataframes\n",
        "    dfs = []\n",
        "\n",
        "    for subgroup, subgroup_df in df.groupby('subgroup'):\n",
        "        print(f\"subgroup: {subgroup} shape[0]: {subgroup_df.shape[0]}\")\n",
        "        subgroup_df = subgroup_df.sort_values(entity_id, ascending=False).reset_index(drop=True)\n",
        "        subgroup_df[columns] = subgroup_df[columns].astype(str)  # Clean up NaN\n",
        "        subgroup_df['combined'] = subgroup_df[columns].agg(' '.join, axis=1)  # TF/IDF documents\n",
        "        subgroup_df['combined'] = subgroup_df['combined'].str.upper()\n",
        "        subgroup_df['dup_id'] = subgroup_df[entity_id]\n",
        "        subgroup_df['dup_flag'] = 0\n",
        "        subgroup_df['score'] = 0.0\n",
        "\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = vectorizer.fit_transform(subgroup_df['combined'])\n",
        "        nbrs = NearestNeighbors(radius=threshold, metric='cosine').fit(tfidf_matrix)\n",
        "        distances, indices = nbrs.radius_neighbors(tfidf_matrix)\n",
        "\n",
        "        for i, (dist, idx) in enumerate(zip(distances, indices)):\n",
        "            for d, j in zip(dist, idx):\n",
        "                if i < j:  # only compare with what comes after\n",
        "                    similarity_score = 1 - d  # Convert distance to similarity\n",
        "                    if similarity_score > threshold:\n",
        "                        if subgroup_df.loc[i, 'dup_flag'] == 0:\n",
        "                            subgroup_df.loc[i, 'dup_flag'] = 2\n",
        "                        subgroup_df.loc[j, 'dup_flag'] = 1\n",
        "                        if subgroup_df.loc[j, 'dup_id'] == subgroup_df.loc[j, entity_id]:\n",
        "                            subgroup_df.loc[j, 'dup_id'] = subgroup_df.loc[i, entity_id]\n",
        "                            if subgroup_df.loc[i, 'score'] == 0.0:\n",
        "                                subgroup_df.loc[i, 'score'] = 1.0\n",
        "                            subgroup_df.loc[j, 'score'] = round(similarity_score, 3)\n",
        "\n",
        "        # Add processed subgroup to the list\n",
        "        dfs.append(subgroup_df)\n",
        "\n",
        "    # Concatenate all dataframes in the list at once\n",
        "    results_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Drop the temporary 'subgroup' column from the results dataframe\n",
        "    results_df.drop(columns=['subgroup'], inplace=True)\n",
        "\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "wDoYnMdh5QKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _crm_templates"
      ],
      "metadata": {
        "id": "rgQkWSHE0Me8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_crm_templates = {\n",
        "    \"donorperfect\": {\n",
        "        \"record\": \"https://www.donorperfect.net/prod/donor.asp\"\n",
        "            + \"?donor_id={entity_id}\",\n",
        "        \"merge\": \"https://www.donorperfect.net/prod/combinedonorsStart.asp\"\n",
        "            + \"?donor_id1={entity_id}&donor_id2={dup_id}&combineType=Advanced\"\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "QUB6bBMQu8vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _excel_redirect()"
      ],
      "metadata": {
        "id": "soRPGbSWT9Bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _excel_redirect(url):\n",
        "    redirector = \"https://benevolentmachines.org/excel_redirect.html\"\n",
        "    url = redirector + '?page=' + url.replace('?',\"&\")\n",
        "    return url"
      ],
      "metadata": {
        "id": "igOhXExTS0JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _output_excel()"
      ],
      "metadata": {
        "id": "pmphUFtRj4-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "!pip install XlsxWriter"
      ],
      "metadata": {
        "id": "e_3-7rjjca_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _output_excel(\n",
        "    pandas_dataframe,\n",
        "    entity_id = 'entity_id',\n",
        "    compare = ['name', 'address', 'city', 'state'],\n",
        "    crm = None,\n",
        "):\n",
        "    import pandas as pd\n",
        "    import xlsxwriter\n",
        "\n",
        "    # organize the columns\n",
        "    initial = ['dup_flag', entity_id, 'dup_id', 'score']\n",
        "    reference = []  # additional fields\n",
        "    xdf = pandas_dataframe\n",
        "    for col in xdf.columns: # extract references for the spreadsheet\n",
        "        if col not in initial and col not in compare:\n",
        "            if col != 'rownum' and col != 'combined':\n",
        "                reference.append(col)\n",
        "    ordered = initial + compare + reference\n",
        "    if crm is not None:\n",
        "        crm = crm.lower()\n",
        "\n",
        "    # prepare the dataframe for the spreadsheet\n",
        "    xdf = xdf[xdf['dup_flag'] > 0] # the spreadsheet only includes duplicates\n",
        "    xdf = xdf[ordered] # the order that will appear in the spreadsheet\n",
        "    xdf = xdf.sort_values(['dup_id','dup_flag'], ascending=[False, False])\n",
        "    xdf = xdf.reset_index(drop=True) # renumber after sorting\n",
        "    xdf = xdf.fillna('') # important\n",
        "    xdf.rename(columns={'dup_flag': 'type'}, inplace=True)\n",
        "    xdf['type'] = xdf['type'].map({2: 'last', 1: 'prior'}).astype(str)\n",
        "    crm_flag = False\n",
        "    if crm is not None:\n",
        "        crm_flag = True\n",
        "        xdf.insert(loc=3, column='action', value='')\n",
        "\n",
        "    excel_file = 'near_duplicates.xlsx'\n",
        "    writer = pd.ExcelWriter(excel_file, engine='xlsxwriter')\n",
        "\n",
        "    xdf.to_excel(writer, index=False, sheet_name='Sheet1', startrow=1, header=False)\n",
        "    workbook = writer.book\n",
        "    worksheet = writer.sheets['Sheet1']\n",
        "\n",
        "    # Custom formats\n",
        "    header_grey_left = workbook.add_format(\n",
        "        {'bg_color': '#D3D3D3', 'bold': True, 'align': 'left'})\n",
        "    header_grey_right = workbook.add_format(\n",
        "        {'bg_color': '#D3D3D3', 'bold': True, 'align': 'right'})\n",
        "    header_green_left = workbook.add_format(\n",
        "        {'bg_color': '#B8D7A3', 'bold': True, 'align': 'left'})\n",
        "\n",
        "    last_blue_left = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'left'})\n",
        "    last_blue_right = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'right'})\n",
        "    last_blue_right_score = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'right', 'num_format': '0.000'})\n",
        "    last_blue_right_date = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'right', 'num_format': 'yyyy-mm-dd'})\n",
        "    last_blue_right_dec2 = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'right', 'num_format': '#,##0.00'})\n",
        "    last_blue_right_dec3 = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'right', 'num_format': '##0.000'})\n",
        "    last_blue_left_link = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'left',\n",
        "        'font_color': 'blue', 'underline': 1})\n",
        "    last_blue_right_link = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'right',\n",
        "        'font_color': 'blue', 'underline': 1})\n",
        "\n",
        "    prior_left = workbook.add_format({'align': 'left'})\n",
        "    prior_right = workbook.add_format({'align': 'right'})\n",
        "    prior_right_score = workbook.add_format(\n",
        "        {'align': 'right', 'num_format': '0.000'})\n",
        "    prior_right_date = workbook.add_format(\n",
        "        {'align': 'right', 'num_format': 'yyyy-mm-dd'})\n",
        "    prior_right_dec2 = workbook.add_format(\n",
        "        {'align': 'right', 'num_format': '#,##0.00'})\n",
        "    prior_right_dec3 = workbook.add_format(\n",
        "        {'align': 'right', 'num_format': '##0.000'}) # score\n",
        "    prior_left_link = workbook.add_format(\n",
        "        {'align': 'left', 'font_color': 'blue', 'underline': 1})\n",
        "    prior_right_link = workbook.add_format(\n",
        "        {'align': 'right', 'font_color': 'blue', 'underline': 1})\n",
        "\n",
        "    ##### determine column properties\n",
        "    tdf = xdf[0:500] # look at first n rows to extract properties\n",
        "    col_info = {} # info about the columns\n",
        "    for col in tdf.columns:\n",
        "        col_props = {}\n",
        "        is_numeric = is_string = is_date = False\n",
        "        max_decimals = 0\n",
        "        is_numeric = pd.to_numeric(\n",
        "            tdf[col].dropna(), errors='coerce').notnull().all()\n",
        "        if is_numeric:\n",
        "            numeric_series = pd.to_numeric(tdf[col], errors='coerce').dropna()\n",
        "            decimal_lengths = numeric_series.apply(\n",
        "                lambda x: len(str(x).split('.')[1]) if '.' in str(x) else 0)\n",
        "            max_decimals = decimal_lengths.max()\n",
        "        else:\n",
        "            is_date = pd.to_datetime(\n",
        "                tdf[col].dropna(), errors='coerce').notnull().all()\n",
        "            if not is_date:\n",
        "                is_string = tdf[col].dtype == 'object'\n",
        "        col_props['is_numeric'] = is_numeric\n",
        "        col_props['is_string'] = is_string\n",
        "        col_props['is_date'] = is_date\n",
        "        col_props['max_decimals'] = max_decimals\n",
        "        col_info[col] = col_props\n",
        "\n",
        "    # column settings\n",
        "    for num, col in enumerate(tdf.columns):\n",
        "\n",
        "        # column widths\n",
        "        max_len = max(tdf[col].astype(str).map(len).max(), len(col)) + 1\n",
        "        if max_len > 15:\n",
        "            max_len = 15\n",
        "        worksheet.set_column(num, num, max_len)\n",
        "\n",
        "        # custom headers\n",
        "        if col in compare: # comparison columns get a green header\n",
        "            worksheet.write(0, num, col, header_green_left)\n",
        "        else:\n",
        "            if col == 'type':\n",
        "                worksheet.write(0, num, col, header_grey_right)\n",
        "            elif col_info[col]['is_string']:\n",
        "                worksheet.write(0, num, col, header_grey_left)\n",
        "            else:\n",
        "                worksheet.write(0, num, col, header_grey_right)\n",
        "\n",
        "    # cell settings\n",
        "    for idx, row in xdf.iterrows():\n",
        "        row_num = idx + 1  # account for header row\n",
        "        for col_num, col in enumerate(tdf.columns):\n",
        "            if row['type'] == 'last':\n",
        "                if col == 'type':\n",
        "                    worksheet.write(row_num, col_num, row[col], last_blue_right)\n",
        "                elif col == entity_id:\n",
        "                    if crm_flag is True: # hyperlinks\n",
        "                        url = _crm_templates[crm][\"record\"].format(\n",
        "                            entity_id = row[entity_id]\n",
        "                        )\n",
        "                        url = _excel_redirect(url) # excel nonsense\n",
        "                        if col_info[col][\"is_string\"]:\n",
        "                            worksheet.write_url(row_num, col_num, url,\n",
        "                                last_blue_left_link,\n",
        "                                string=str(row[entity_id]))\n",
        "                        else:\n",
        "                            worksheet.write_url(row_num, col_num, url,\n",
        "                                last_blue_right_link,\n",
        "                                string=str(row[entity_id]))\n",
        "                    else:\n",
        "                        if col_info[col][\"is_string\"]:\n",
        "                            worksheet.write(\n",
        "                                row_num, col_num, row[col], last_blue_left)\n",
        "                        else:\n",
        "                            worksheet.write(\n",
        "                                row_num, col_num, row[col], last_blue_right)\n",
        "                elif col == 'action': # blank and blue\n",
        "                    worksheet.write(row_num, col_num, '', last_blue_right)\n",
        "                elif col == 'score': # blank and blue\n",
        "                    worksheet.write(row_num, col_num, '', last_blue_right)\n",
        "                else:\n",
        "                    if col_info[col][\"is_string\"]:\n",
        "                        worksheet.write(row_num, col_num, row[col], last_blue_left)\n",
        "                    elif col_info[col]['is_numeric']:\n",
        "                        if col_info[col]['max_decimals'] == 2:\n",
        "                            worksheet.write(row_num, col_num, row[col], last_blue_right_dec2)\n",
        "                        else:\n",
        "                            worksheet.write(row_num, col_num, row[col], last_blue_right)\n",
        "                    elif col_info[col]['is_date']:\n",
        "                        worksheet.write(row_num, col_num, row[col], last_blue_right_date)\n",
        "                    else: # other?\n",
        "                        worksheet.write(row_num, col_num, row[col], last_blue_right)\n",
        "            else: # prior\n",
        "                if col == 'type':\n",
        "                    worksheet.write(row_num, col_num, row[col], prior_right)\n",
        "                elif col == entity_id:\n",
        "                    if crm_flag is True: # hyperlinks\n",
        "                        url = _crm_templates[crm][\"record\"].format(\n",
        "                            entity_id = row[entity_id]\n",
        "                        )\n",
        "                        url = _excel_redirect(url) # excel nonsense\n",
        "                        if col_info[col][\"is_string\"]:\n",
        "                            worksheet.write_url(row_num, col_num, url,\n",
        "                                prior_left_link, string=str(row[entity_id]))\n",
        "                        else:\n",
        "                            worksheet.write_url(row_num, col_num, url,\n",
        "                                prior_right_link, string=str(row[entity_id]))\n",
        "                    else:\n",
        "                        if col_info[col][\"is_string\"]:\n",
        "                            worksheet.write(\n",
        "                                row_num, col_num, row[col], prior_left)\n",
        "                        else:\n",
        "                            worksheet.write(\n",
        "                                row_num, col_num, row[col], prior_right)\n",
        "                elif col == 'action':\n",
        "                    if crm_flag is True:\n",
        "                        url = _crm_templates[crm][\"merge\"].format(\n",
        "                            entity_id = row[entity_id],\n",
        "                            dup_id = row[\"dup_id\"]\n",
        "                        )\n",
        "                        url = _excel_redirect(url) # excel nonsense\n",
        "                        worksheet.write_url(row_num, col_num, url,\n",
        "                            prior_left_link, string='merge')\n",
        "                elif col == 'score':\n",
        "                    worksheet.write(row_num, col_num, row[col], prior_right_dec3)\n",
        "                else:\n",
        "                    if col_info[col][\"is_string\"]:\n",
        "                        #print(col)\n",
        "                        worksheet.write(row_num, col_num, row[col], prior_left)\n",
        "                    elif col_info[col]['is_numeric']:\n",
        "                        if col_info[col]['max_decimals'] == 2:\n",
        "                            worksheet.write(row_num, col_num, row[col], prior_right_dec2)\n",
        "                        else:\n",
        "                            worksheet.write(row_num, col_num, row[col], prior_right)\n",
        "                    elif col_info[col]['is_date']:\n",
        "                        worksheet.write(row_num, col_num, row[col], prior_right_date)\n",
        "                    else: # other?\n",
        "                        worksheet.write(row_num, col_num, row[col], prior_right)\n",
        "\n",
        "    worksheet.freeze_panes(1, 4)\n",
        "    writer.close()\n",
        "    return xdf"
      ],
      "metadata": {
        "id": "xvgbS_wGXlMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run()"
      ],
      "metadata": {
        "id": "xufeH01u9B5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run(\n",
        "    input_data, # file (xls,xlsx,csv) or dataframe\n",
        "    entity_id='entity_id',\n",
        "    compare = ['name', 'address', 'city', 'state'],\n",
        "    crm_links = None,\n",
        "    score_minimum = .67,\n",
        "    filter_column = None, # 'gift_total'\n",
        "    filter_minimum = None, # 2_000\n",
        "):\n",
        "    print(\"loading data\")\n",
        "    df = _load_data(input_data) # load file\n",
        "    #df = df[0:10000]\n",
        "    print(\"processing algorithms\")\n",
        "    df = _tfidf_neighbors(df, entity_id, compare) # TF/IDF with neighbor cosines\n",
        "\n",
        "    if filter_column is not None and filter_minimum is not None:\n",
        "        # filter on duplications involving filter_field at a filter_minimum\n",
        "        dup_ids = list(\n",
        "            df[df[filter_column] >= filter_minimum]['dup_id'].unique())\n",
        "        df = df[df['dup_id'].isin(dup_ids)]\n",
        "\n",
        "    print(\"outputting results\")\n",
        "    _output_excel(df, entity_id, compare, crm_links)\n",
        "    return\n"
      ],
      "metadata": {
        "id": "JnHWuUYC9RF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTS"
      ],
      "metadata": {
        "id": "7ZybEnjd9Lnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CRM Export File"
      ],
      "metadata": {
        "id": "OiberZ4D30rP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "run(\n",
        "    input_data = \"donors.csv\", # exported from the CRM\n",
        "    entity_id = \"donor_id\", # unique id\n",
        "    compare = ['first_name', 'last_name', 'address', 'city', 'state'],\n",
        "    crm = \"donorperfect\"\n",
        ")"
      ],
      "metadata": {
        "id": "bi80Ma1e3LBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "gdf = _load_data('donors.csv')\n",
        "df = _tfidf_neighbors(\n",
        "    gdf, threshold=.60,\n",
        "    entity_id='donor_id',\n",
        "    columns=['first_name', 'last_name', 'address', 'city', 'state'])"
      ],
      "metadata": {
        "id": "cYdWQYunRyFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "df.shape"
      ],
      "metadata": {
        "id": "lEGDa7FXwfQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "\n",
        "print(df[crit].shape)\n",
        "df[crit][['donor_id','dup_id','dup_flag','score',\n",
        "          'first_name', 'last_name', 'address', 'city', 'state','zip','gift_total','last_contrib_date']].sort_values(\n",
        "              ['dup_id','dup_flag'], ascending=[False, False])"
      ],
      "metadata": {
        "id": "JyAWMiAZZQWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "import pandas as pd\n",
        "df['prefix'] = df['last_name'].str[:2]\n",
        "groups_with_diff_city_prefix = df.groupby('dup_id').filter(lambda x: x['prefix'].nunique() > 1)\n",
        "result = groups_with_diff_city_prefix.sort_values(by=['dup_id', 'prefix'])\n",
        "display(result[['donor_id','dup_id','dup_flag','score',\n",
        "          'first_name', 'last_name', 'address', 'city', 'state','zip','gift_total','last_contrib_date']]\n",
        "        .sort_values(['dup_id','dup_flag'], ascending=False))"
      ],
      "metadata": {
        "id": "J3QgABOm86Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Data"
      ],
      "metadata": {
        "id": "urQeDlNKXas3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _first_name_variations"
      ],
      "metadata": {
        "id": "BgmwhJFF6mhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "_first_name_variations = {\n",
        "    'Dave': 'David',\n",
        "    'Joe': 'Joseph',\n",
        "    'Elizabeth': 'Betty',\n",
        "    'Bill': 'William',\n",
        "    'Bob': 'Robert',\n",
        "    'Tom': 'Thomas',\n",
        "    'Dick': 'Richard',\n",
        "    'Jim': 'James',\n",
        "    'Jenny': 'Jennifer',\n",
        "    'Kathy': 'Katherine',\n",
        "    'Sue': 'Susan',\n",
        "    'Maggie': 'Margaret',\n",
        "    'Harry': 'Harold',\n",
        "    'Ron': 'Ronald',\n",
        "    'Nick': 'Nicholas',\n",
        "    'Patty': 'Patricia',\n",
        "    'Chris': 'Christopher',\n",
        "    'Mike': 'Michael',\n",
        "    'Steve': 'Steven',\n",
        "    'Andy': 'Andrew',\n",
        "    'Matty': 'Matthew',\n",
        "    'Sam': 'Samuel',\n",
        "    'Tony': 'Anthony',\n",
        "}"
      ],
      "metadata": {
        "id": "zVPgVYRmw-v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _first_name_variation()"
      ],
      "metadata": {
        "id": "ihgJUudWDTIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "def _first_name_variation(full_name, chance=40):\n",
        "    import random\n",
        "    name_parts = full_name.split()\n",
        "    first_name = name_parts[0]\n",
        "    if random.random() < (chance / 100.0):\n",
        "        if first_name in _first_name_variations:\n",
        "            first_name = _first_name_variations[first_name]\n",
        "        else:\n",
        "            for key, value in _first_name_variations.items():\n",
        "                if first_name == value:\n",
        "                    first_name = key\n",
        "                    break\n",
        "    return ' '.join([first_name] + name_parts[1:])\n",
        "\n",
        "#print(_first_name_variation(\"Joe Satriani\",chance=100))\n",
        "#print(_first_name_variation(\"David Bowie\",chance=100))"
      ],
      "metadata": {
        "id": "laWrKB3oyKdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _last_name_transposition()"
      ],
      "metadata": {
        "id": "aHeL4BiU6soR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "def _last_name_transposition(full_name, chance=1):\n",
        "    import random\n",
        "    name_parts = full_name.split()\n",
        "    if len(name_parts) > 1 and random.random() < (chance / 100.0):\n",
        "        last_name = name_parts[-1]\n",
        "        if len(last_name) > 4:\n",
        "            i = random.randint(1, len(last_name) - 2)\n",
        "            last_name_chars = list(last_name)\n",
        "            last_name_chars[i], last_name_chars[i+1] = \\\n",
        "              last_name_chars[i+1], last_name_chars[i]\n",
        "            name_parts[-1] = ''.join(last_name_chars)\n",
        "    return ' '.join(name_parts)\n",
        "#print(_last_name_transposition(\"Jon Anderson\", chance=100))"
      ],
      "metadata": {
        "id": "AO5lXRJN2KGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _generate_test_data()"
      ],
      "metadata": {
        "id": "kWixweujGFWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "!pip install mimesis # used for generating test data"
      ],
      "metadata": {
        "id": "aUFCr1OM_91h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "def _generate_test_data(n):\n",
        "\n",
        "    from mimesis import Person, Address, Finance\n",
        "    import pandas as pd\n",
        "    import random\n",
        "\n",
        "    data = []\n",
        "    entity_id = 0\n",
        "\n",
        "    person = Person('en')\n",
        "    address_gen = Address('en')\n",
        "    finance = Finance('en')\n",
        "\n",
        "    for _ in range(n):\n",
        "        entity_id += 1\n",
        "        is_person = True\n",
        "        name = person.full_name()\n",
        "        if random.random() < (10.0 / 100.0):\n",
        "            is_person = False\n",
        "            name = finance.company()\n",
        "        street_address = address_gen.address()\n",
        "        city = address_gen.city()\n",
        "        state = address_gen.state(abbr=True)\n",
        "        label = 0  # not a duplicate\n",
        "        data.append([entity_id, name, street_address, city, state, label])\n",
        "        if random.random() < (1.0 / 100.0): # exact duplicate\n",
        "            entity_id += 1\n",
        "            label = 1\n",
        "            data[-1][-1] = label # update the prior label\n",
        "            data.append([entity_id, name, street_address, city, state, label])\n",
        "        if is_person:\n",
        "            dup_name = _first_name_variation(name, chance=30)\n",
        "            dup_name = _last_name_transposition(dup_name, chance=1)\n",
        "            if name != dup_name: # fuzzy duplicate\n",
        "                entity_id += 1\n",
        "                label = 1\n",
        "                data[-1][-1] = label # update the prior label\n",
        "                data.append([\n",
        "                    entity_id, dup_name, street_address, city, state, label])\n",
        "    random.shuffle(data) # mix them up!\n",
        "    df = pd.DataFrame(data,\n",
        "        columns=['entity_id', 'name', 'address', 'city', 'state', 'label'])\n",
        "    df['entity_id'] = df.index + 1 # renumber to shuffled order\n",
        "    df.to_csv('test.csv', index=False) # test data for passing to _load_data()\n",
        "\n",
        "    return df\n",
        "#_generate_test_data(1000)"
      ],
      "metadata": {
        "id": "TKpjq_Q_nKzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "gdf = _generate_test_data(10000)"
      ],
      "metadata": {
        "id": "vXFcHUrBSsCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "df = _tfidf_neighbors_subgrouped(\n",
        "    gdf, threshold=.60, subgroup_column=\"city\"\n",
        "    )"
      ],
      "metadata": {
        "id": "7NzXvMPFTHWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "\"\"\"\n",
        "class m: pass\n",
        "gdf = _generate_test_data(1000)\n",
        "glabel = gdf['label'] > 0\n",
        "print('test labels:', gdf[glabel].shape[0])\n",
        "df = _tfidf_neighbors(gdf, threshold=.75)\n",
        "\n",
        "#print('tfidf labels:', df[df['label'] > 0].shape[0])\n",
        "#print('tfidf labels found:', df[(df['label'] > 0) & (df['dup_flag'] > 0)].shape[0])\n",
        "#print('tfidf found:', df[(df['dup_flag'] > 0)].shape[0])\n",
        "\"\"\"\n",
        "crit = (df['dup_flag'] > 0) # | (df['label'] > 0)\n",
        "print(df[crit].shape)\n",
        "df[crit][['entity_id','dup_id','dup_flag','score','label',\n",
        "          'name','address','city','state','combined']].sort_values(\n",
        "              ['dup_id','dup_flag'], ascending=[False, False])"
      ],
      "metadata": {
        "id": "xE-rkFRbZOhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "# prep generated data\n",
        "compare = ['name', 'address', 'city', 'state']\n",
        "reference = ['label']\n",
        "columns_order = ['entity_id', 'dup_id', 'dup_flag', 'score'] + compare + reference\n",
        "xdf = df[crit][columns_order].sort_values(\n",
        "              ['dup_id','dup_flag'], ascending=[False, False])\n",
        "xdf = xdf[0:500].reset_index(drop=True)\n",
        "xdf.shape[0]"
      ],
      "metadata": {
        "id": "Mc7jngsZczqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "# ['first_name', 'last_name', 'address', 'city', 'state']\n",
        "# prep exported  data\n",
        "crm_flag = True\n",
        "entity_id = 'donor_id'\n",
        "\n",
        "compare = ['first_name', 'last_name', 'address', 'city', 'state']\n",
        "reference = ['gift_total', 'created_by', 'last_contrib_date', 'last_contrib_amt', 'zip']\n",
        "columns_order = initial + compare + reference\n",
        "xdf = df[crit][columns_order].sort_values(['dup_id','dup_flag'], ascending=[False, False])\n",
        "xdf = xdf[0:500].reset_index(drop=True)\n",
        "xdf.rename(columns={'dup_flag': 'type'}, inplace=True)\n",
        "xdf['type'] = xdf['type'].map({2: 'last', 1: 'prior'}).astype(str)\n",
        "if crm_flag is True:\n",
        "    xdf.insert(loc=3, column='action', value='')\n",
        "output_columns = xdf.columns.tolist() # direct mapping to output\n",
        "print(output_columns)\n",
        "xdf #.shape[0]"
      ],
      "metadata": {
        "id": "nbHVYiZfade1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyses"
      ],
      "metadata": {
        "id": "dMZCwsEilNau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _rows_vs_time_plot()"
      ],
      "metadata": {
        "id": "6naBwTMap3nL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "def _rows_vs_time_plot():\n",
        "    import pandas as pd\n",
        "    import plotly.express as px\n",
        "    import numpy as np\n",
        "    cdf = pd.DataFrame()\n",
        "    cdf['rows'] = list(range(10_000, 1_010_000, 10_000))\n",
        "    cdf['compares'] = cdf['rows'] * (cdf['rows'] - 1) / 2\n",
        "    cdf['time'] = cdf['compares'] / 21_766_643  # seconds\n",
        "    cdf['time_delta'] = pd.to_timedelta(cdf['time'], unit='s')\n",
        "    cdf['time_seconds'] = cdf['time_delta'].dt.total_seconds()\n",
        "    max_hours = cdf['time_delta'].dt.total_seconds().max() / 3600\n",
        "    tick_interval_minutes = 15\n",
        "    tick_interval = pd.Timedelta(minutes=tick_interval_minutes)\n",
        "    tick_vals = [i*tick_interval for i in range(int(max_hours*6) + 1)]\n",
        "    tick_vals_seconds = [tick.total_seconds() for tick in tick_vals]\n",
        "    tick_labels = [(tick.total_seconds() // 3600,\n",
        "    (tick.total_seconds() % 3600) // 60) for tick in tick_vals]\n",
        "    tick_text = [f\"{int(h):02d}:{int(m):02d}\" for h, m in tick_labels]\n",
        "    fig = px.line(cdf, x='rows', y='time_seconds', labels={'time_delta': 'Time (hh:mm)'})\n",
        "    fig.update_traces(textposition='top center')\n",
        "    fig.update_yaxes(tickvals=tick_vals_seconds, ticktext=tick_text)\n",
        "    fig.update_xaxes(tickvals=list(range(50_000, 1_010_000, 50_000)), tickangle=-90)\n",
        "    fig.update_layout(yaxis_title=\"Time (hh:mm)\", xaxis_title=\"Number of Rows\")\n",
        "    fig.show()\n",
        "_rows_vs_time_plot()"
      ],
      "metadata": {
        "id": "dJcfSHf6RYhB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
