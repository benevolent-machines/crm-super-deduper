{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPKG+/2eax1/PryiuYuDszM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benevolent-machines/crm-super-deduper/blob/main/docs/main-module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CRM Super Deduper Module\n",
        "\n",
        "See our repository for more information:\n",
        "\n",
        "https://github.com/benevolent-machines/crm-super-deduper\n"
      ],
      "metadata": {
        "id": "ACWPuMCsWng7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _temp\n",
        "A hidden global namespace for storing temporary copies of local variables that can be inspected outside of their scope.  "
      ],
      "metadata": {
        "id": "WI5l4BeZ-DZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class _temp: pass"
      ],
      "metadata": {
        "id": "Zovdihzb93KP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _time()\n",
        "\n",
        "A helper function that returns a string with the current time and elapsed time since the last call."
      ],
      "metadata": {
        "id": "3NfxzOqsd1ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_last = None\n",
        "def _time():\n",
        "    from datetime import datetime\n",
        "    global _last\n",
        "    now = datetime.now()\n",
        "    header = \"\"\n",
        "    if _last is None:\n",
        "        elapsed = \"00:00.000\"  # Initial call, no elapsed time\n",
        "    else:\n",
        "        delta = now - _last\n",
        "        minutes, seconds = divmod(delta.total_seconds(), 60)\n",
        "        milliseconds = delta.microseconds / 1000\n",
        "        elapsed = f\"{int(minutes):02}:{int(seconds):02}.{int(milliseconds):03}\"\n",
        "    _last = now\n",
        "    f_time = now.strftime('%M:%S.%f')[:-3]\n",
        "    return f\"{f_time} {elapsed}\""
      ],
      "metadata": {
        "id": "giTtxbhjdDUd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "if False:\n",
        "    import time\n",
        "    print(_time(),'first')  # First call, elapsed will be \"00:00.000\"\n",
        "    time.sleep(1.234)  # Sleep for a bit over a second to see the change\n",
        "    print(_time(), 'next')  # Second call, should show elapsed time since first call"
      ],
      "metadata": {
        "id": "SQ1iFwHRFfRf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _similar()\n",
        "A helper function to quickly assess the similarity between two strings, useful for refining machine learning recall into higher precision levels. This Python code was adapted from a T-SQL function found here:\n",
        "\n",
        "https://www.sqlservercentral.com/articles/fuzzy-search"
      ],
      "metadata": {
        "id": "yvcZNeTKL_mI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _similar(s1, s2):\n",
        "    if len(s1) > len(s2):\n",
        "        (s1, s2) = (s2, s1) # easy switch in python!\n",
        "    k = len(s1)\n",
        "    if k < 2:\n",
        "        return 0.0\n",
        "    else:\n",
        "        j = 0\n",
        "        for i in range(len(s1) - 1):\n",
        "            substring = s1[i:i+2]\n",
        "            if substring in s2:\n",
        "                j += 1\n",
        "        return (j / (k-1)) * 100.0"
      ],
      "metadata": {
        "id": "pLRFFcu3ZbT7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "if False:\n",
        "    print(f\"{_similar('roger','susan')}\")\n",
        "    print(f\"{_similar('example','example')}\")"
      ],
      "metadata": {
        "id": "H-NEwJfc8lV5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _load_data()\n",
        "A helper function to load data from either a csv or excel file."
      ],
      "metadata": {
        "id": "0xtY_qFDYhZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _load_data(input_data):\n",
        "    import pandas as pd\n",
        "\n",
        "    if isinstance(input_data, str):\n",
        "        file_extension = input_data.split('.')[-1].lower()\n",
        "        if file_extension == 'csv':\n",
        "            df = pd.read_csv(input_data)\n",
        "        elif file_extension in ['xlsx', 'xls']:\n",
        "            df = pd.read_excel(input_data)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"load_data(): Unsupported file type: {file_extension}\")\n",
        "    elif isinstance(input_data, pd.DataFrame):\n",
        "        df = input_data\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"load_data(): Unsupported file type: {file_extension}\")\n",
        "    return df\n",
        "\n",
        "#_load_data(\"test.csv\")"
      ],
      "metadata": {
        "id": "vS0K-IvSY6_D"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _regroup_pass()\n",
        "A function designed to regroup rows during a deduplication pass for more even distribution, based on a specified threshold."
      ],
      "metadata": {
        "id": "l5vI5vmbXxj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _regroup_pass(df, col, num, threshold=2000):\n",
        "\n",
        "    # Slice the first `num` characters of `col` for grouping\n",
        "    sliced_col = df[col].str[:num].str.upper()\n",
        "\n",
        "    # Read sliced_col counts in descending order\n",
        "    col_counts = sliced_col.value_counts().sort_values(ascending=False)\n",
        "    accumulated = 0\n",
        "    group_id = 1  # Start sequence number for small groups\n",
        "    group_mapping = {}\n",
        "\n",
        "    for col_name, count in col_counts.items():\n",
        "        if count >= threshold:\n",
        "            # Categories larger than the threshold keep their original name\n",
        "            group_mapping[col_name] = col_name\n",
        "        else:\n",
        "            # Accumulate small categories until they meet the threshold\n",
        "            if accumulated + count <= threshold:\n",
        "                group_mapping[col_name] = f'_{group_id}'\n",
        "                accumulated += count\n",
        "            else:\n",
        "                # Once the threshold is exceeded, start a new group\n",
        "                group_id += 1\n",
        "                group_mapping[col_name] = f'_{group_id}'\n",
        "                accumulated = count  # Reset accumulated count for the new group\n",
        "\n",
        "    # Apply the group mapping based on the sliced_col values\n",
        "    df['group'] = sliced_col.map(group_mapping)\n",
        "    return df"
      ],
      "metadata": {
        "id": "IVV8uhfR_q38"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "if False:\n",
        "    df = _regroup_pass(df, 'state', 2, 3000)\n",
        "    group_sizes = df.groupby('group').size()\n",
        "    # Display count, maximum, and average of group sizes\n",
        "    print(f\"Total Groups: {group_sizes.count()}\")\n",
        "    print(f\"Maximum Group Size: {group_sizes.max()}\")\n",
        "    print(f\"Median Group Size: {group_sizes.median()}\")\n",
        "    df['group'].value_counts().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "4guPDKJYYD3F"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _find_duplicates()\n",
        "The main algorithm function for finding duplicates in large datasets."
      ],
      "metadata": {
        "id": "PYcf0ALXPtxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _find_duplicates(\n",
        "    pandas_dataframe, # required\n",
        "    entity_id = 'entity_id',\n",
        "    search_columns = ['name', 'address', 'city', 'state'],\n",
        "    dedupe_passes = ['ALL'],  # ['state:2', 'name:3']\n",
        "    similar_column = None,\n",
        "    score_minimum = 0.7,\n",
        "):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "    df = pandas_dataframe\n",
        "\n",
        "    if dedupe_passes is None:\n",
        "        dedupe_passes = ['ALL']\n",
        "    elif isinstance(dedupe_passes, str):\n",
        "        dedupe_passes = [dedupe_passes]\n",
        "    elif not isinstance(dedupe_passes, list):\n",
        "        raise ValueError(\"invalid dedupe_passes parameter\")\n",
        "\n",
        "    pass_df_list = []\n",
        "    df['pass'] = None\n",
        "    df['group'] = None # initialize to avoid pandas warnings\n",
        "\n",
        "    for dedupe_pass in dedupe_passes: # ['state:2', 'city'] or 'ALL'\n",
        "        print(f\"{_time()} pass: {dedupe_pass}\")\n",
        "        pass_list = dedupe_pass.split(':')\n",
        "        pass_col = pass_list[0]\n",
        "        pass_len = int(pass_list[1]) if len(pass_list) > 1 else 2\n",
        "        df['pass'] = dedupe_pass\n",
        "        if pass_col == 'ALL':\n",
        "            df['group'] = 'ALL'\n",
        "        else:\n",
        "            df[pass_col] = df[pass_col].astype(str)\n",
        "            df = _regroup_pass(df, pass_col, pass_len) # updates df['group']\n",
        "\n",
        "        gdf_list = []\n",
        "\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            stop_words=None,\n",
        "            ngram_range=(1, 1), min_df=2, max_df=0.8)\n",
        "\n",
        "        for group, gdf in df.groupby('group'):\n",
        "            print(f\"{_time()} group: {group} shape[0]: {gdf.shape[0]}\")\n",
        "            gdf = gdf.sort_values(entity_id, ascending=False)\n",
        "            gdf = gdf.reset_index(drop=True)\n",
        "            gdf[search_columns] = gdf[search_columns].astype(str)\n",
        "            gdf['combined'] = gdf[search_columns].agg(' '.join, axis=1)\n",
        "            gdf['combined'] = gdf['combined'].str.upper()\n",
        "            gdf['last_id'] = gdf[entity_id]\n",
        "            gdf['dup_flag'] = 0\n",
        "            gdf['score'] = 0.0\n",
        "\n",
        "            tfidf_matrix = vectorizer.fit_transform(gdf['combined'])\n",
        "            print(f\"{_time()} vectorizer.fit_transform() completed\")\n",
        "\n",
        "            nbrs = NearestNeighbors(\n",
        "                radius=0.33, metric='cosine', algorithm='brute')\n",
        "            print(f\"{_time()} NearestNeighbors() completed\")\n",
        "\n",
        "            nbrs.fit(tfidf_matrix)\n",
        "            print(f\"{_time()} nbrs.fit() completed\")\n",
        "\n",
        "            distances, indices = nbrs.radius_neighbors(tfidf_matrix)\n",
        "            print(f\"{_time()} nbrs.radius_neighbors() completed\")\n",
        "\n",
        "            for i, (dist, idx) in enumerate(zip(distances, indices)):\n",
        "                for d, j in zip(dist, idx):\n",
        "                    if i < j:  # only compare with what comes after\n",
        "                        if (similar_column is not None\n",
        "                            and _similar(gdf.loc[i, similar_column],\n",
        "                                         gdf.loc[j, similar_column]) < 50\n",
        "                        ):\n",
        "                            continue; # not similar\n",
        "                        score = 1 - d  # Convert distance to similarity\n",
        "                        if score > score_minimum:\n",
        "                            if gdf.loc[i, 'dup_flag'] == 0:\n",
        "                                gdf.loc[i, 'dup_flag'] = 2\n",
        "                            gdf.loc[j, 'dup_flag'] = 1\n",
        "                            if gdf.loc[j, 'last_id'] == gdf.loc[j, entity_id]:\n",
        "                                gdf.loc[j, 'last_id'] = gdf.loc[i, entity_id]\n",
        "                                if gdf.loc[i, 'score'] == 0.0:\n",
        "                                    gdf.loc[i, 'score'] = 1.0\n",
        "                                gdf.loc[j, 'score'] = round(score, 3)\n",
        "\n",
        "            gdf_list.append(gdf)\n",
        "\n",
        "        pass_df = pd.concat(gdf_list, ignore_index=True)\n",
        "        #pass_df.drop(columns=['group'], inplace=True)\n",
        "\n",
        "        pass_df_list.append(pass_df[pass_df['dup_flag'] > 0]) # collect dups\n",
        "        df = pass_df[pass_df['dup_flag'] == 0].copy() # drop dups in next pass\n",
        "\n",
        "    results_df = pd.concat(pass_df_list, ignore_index=True)\n",
        "    #results_df.drop(columns=['group'], inplace=True)\n",
        "    #results_df = results_df.copy()\n",
        "    #m.df = df.copy()\n",
        "    all_df = pd.concat([df, results_df], ignore_index=True)\n",
        "    #all_df.to_csv(\"labeled_out.csv\")\n",
        "\n",
        "    _temp.audit_df = all_df\n",
        "\n",
        "    return all_df"
      ],
      "metadata": {
        "id": "wDoYnMdh5QKX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _excel_redirect()\n",
        "A helper function to ensure that links in Excel can access CRM functions without being sent to a login screen each time."
      ],
      "metadata": {
        "id": "soRPGbSWT9Bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _excel_redirect(url):\n",
        "    redirector = \"https://benevolentmachines.org/excel_redirect.html\"\n",
        "    url = redirector + '?page=' + url.replace('?',\"&\")\n",
        "    return url"
      ],
      "metadata": {
        "id": "igOhXExTS0JA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _link_templates\n",
        "A dictionary of dictionaries with templates used to construct links in the Excel output file.  "
      ],
      "metadata": {
        "id": "rgQkWSHE0Me8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_link_templates = {\n",
        "    \"donorperfect\": {\n",
        "        \"record\": \"https://www.donorperfect.net/prod/donor.asp\"\n",
        "            + \"?donor_id={entity_id}\",\n",
        "        \"merge\": \"https://www.donorperfect.net/prod/combinedonorsStart.asp\"\n",
        "            + \"?donor_id1={entity_id}&donor_id2={last_id}&combineType=Advanced\"\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "QUB6bBMQu8vz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _output_excel()\n",
        "The function used to output found duplicates in a formatted Excel document.  "
      ],
      "metadata": {
        "id": "pmphUFtRj4-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test - The invoking context must install this package and datamachine to work.\n",
        "!pip install XlsxWriter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_3-7rjjca_K",
        "outputId": "e412b4d1-7618-43e4-882b-87d106e94c28"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting XlsxWriter\n",
            "  Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: XlsxWriter\n",
            "Successfully installed XlsxWriter-3.1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _output_excel(\n",
        "    pandas_dataframe,\n",
        "    entity_id = 'entity_id',\n",
        "    search_columns = ['name', 'address', 'city', 'state'],\n",
        "    link_templates_key = None,\n",
        "    output_all = False, # True outputs all records, False is just the dups\n",
        "):\n",
        "    import pandas as pd\n",
        "    import xlsxwriter\n",
        "\n",
        "    # organize the columns\n",
        "    initial = ['dup_flag', entity_id, 'last_id', 'score']\n",
        "    reference = []  # additional fields\n",
        "    xdf = pandas_dataframe\n",
        "    for col in xdf.columns: # extract references for the spreadsheet\n",
        "        if col not in initial and col not in search_columns:\n",
        "            if col != 'rownum' and col != 'combined':\n",
        "                reference.append(col)\n",
        "    ordered = initial + search_columns + reference\n",
        "    if link_templates_key is not None:\n",
        "        link_templates_key = link_templates_key.lower()\n",
        "\n",
        "    # prepare the dataframe for the spreadsheet\n",
        "    if output_all == False:\n",
        "        xdf = xdf[xdf['dup_flag'] > 0] # the spreadsheet only includes duplicates\n",
        "    xdf = xdf[ordered] # the order that will appear in the spreadsheet\n",
        "    xdf = xdf.sort_values(['last_id','dup_flag'], ascending=[False, False])\n",
        "    xdf = xdf.reset_index(drop=True) # renumber after sorting\n",
        "    xdf = xdf.fillna('') # important\n",
        "    xdf.rename(columns={'dup_flag': 'type'}, inplace=True)\n",
        "    xdf['type'] = xdf['type'].map({2: 'last', 1: 'prior'}).astype(str)\n",
        "    xdf['type'] = xdf['type'].str.replace('nan','')\n",
        "\n",
        "    if link_templates_key is not None:\n",
        "        xdf.insert(loc=3, column='action', value='')\n",
        "\n",
        "    excel_file = 'output.xlsx'\n",
        "    writer = pd.ExcelWriter(excel_file, engine='xlsxwriter')\n",
        "\n",
        "    xdf.to_excel(writer, index=False, sheet_name='Sheet1', startrow=1, header=False)\n",
        "    workbook = writer.book\n",
        "    worksheet = writer.sheets['Sheet1']\n",
        "\n",
        "    # Custom formats\n",
        "    header_grey_left = workbook.add_format(\n",
        "        {'bg_color': '#D3D3D3', 'bold': True, 'align': 'left'})\n",
        "    header_grey_right = workbook.add_format(\n",
        "        {'bg_color': '#D3D3D3', 'bold': True, 'align': 'right'})\n",
        "    header_green_left = workbook.add_format(\n",
        "        {'bg_color': '#B8D7A3', 'bold': True, 'align': 'left'})\n",
        "\n",
        "    last_blue_left = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'left'})\n",
        "    last_blue_right = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'right'})\n",
        "    last_blue_right_score = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'right', 'num_format': '0.000'})\n",
        "    last_blue_right_date = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'right'}) #, 'num_format': 'yyyy-mm-dd'})\n",
        "    last_blue_right_dec2 = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'right', 'num_format': '#,##0.00'})\n",
        "    last_blue_right_dec3 = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'right', 'num_format': '##0.000'})\n",
        "    last_blue_left_link = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'left',\n",
        "        'font_color': 'blue', 'underline': 1})\n",
        "    last_blue_right_link = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'right',\n",
        "        'font_color': 'blue', 'underline': 1})\n",
        "\n",
        "    prior_left = workbook.add_format({'align': 'left'})\n",
        "    prior_right = workbook.add_format({'align': 'right'})\n",
        "    prior_right_score = workbook.add_format(\n",
        "        {'align': 'right', 'num_format': '0.000'})\n",
        "    prior_right_date = workbook.add_format(\n",
        "        {'align': 'right', 'num_format': 'yyyy-mm-dd'})\n",
        "    prior_right_dec2 = workbook.add_format(\n",
        "        {'align': 'right', 'num_format': '#,##0.00'})\n",
        "    prior_right_dec3 = workbook.add_format(\n",
        "        {'align': 'right', 'num_format': '##0.000'}) # score\n",
        "    prior_left_link = workbook.add_format(\n",
        "        {'align': 'left', 'font_color': 'blue', 'underline': 1})\n",
        "    prior_right_link = workbook.add_format(\n",
        "        {'align': 'right', 'font_color': 'blue', 'underline': 1})\n",
        "\n",
        "    ##### determine column properties\n",
        "    tdf = xdf[0:500] # look at first n rows to extract properties\n",
        "    col_info = {} # info about the columns\n",
        "    for col in tdf.columns:\n",
        "        col_props = {}\n",
        "        is_numeric = is_string = is_date = False\n",
        "        max_decimals = 0\n",
        "        is_numeric = pd.to_numeric(\n",
        "            tdf[col].dropna(), errors='coerce').notnull().all()\n",
        "        if is_numeric:\n",
        "            numeric_series = pd.to_numeric(tdf[col], errors='coerce').dropna()\n",
        "            decimal_lengths = numeric_series.apply(\n",
        "                lambda x: len(str(x).split('.')[1]) if '.' in str(x) else 0)\n",
        "            max_decimals = decimal_lengths.max()\n",
        "        else:\n",
        "            is_date = pd.to_datetime(\n",
        "                tdf[col].dropna(), errors='coerce').notnull().all()\n",
        "            if not is_date:\n",
        "                is_string = tdf[col].dtype == 'object'\n",
        "        col_props['is_numeric'] = is_numeric\n",
        "        col_props['is_string'] = is_string\n",
        "        col_props['is_date'] = is_date\n",
        "        col_props['max_decimals'] = max_decimals\n",
        "        col_info[col] = col_props\n",
        "\n",
        "    # column settings\n",
        "    for num, col in enumerate(tdf.columns):\n",
        "\n",
        "        # column widths\n",
        "        max_len = max(tdf[col].astype(str).map(len).max(), len(col)) + 1\n",
        "        if max_len > 15:\n",
        "            max_len = 15\n",
        "        worksheet.set_column(num, num, max_len)\n",
        "\n",
        "        # custom headers\n",
        "        if col in search_columns: # comparison columns get a green header\n",
        "            worksheet.write(0, num, col, header_green_left)\n",
        "        else:\n",
        "            if col == 'type':\n",
        "                worksheet.write(0, num, col, header_grey_right)\n",
        "            elif col_info[col]['is_string']:\n",
        "                worksheet.write(0, num, col, header_grey_left)\n",
        "            else:\n",
        "                worksheet.write(0, num, col, header_grey_right)\n",
        "\n",
        "    # cell settings\n",
        "    for idx, row in xdf.iterrows():\n",
        "        row_num = idx + 1  # account for header row\n",
        "        for col_num, col in enumerate(tdf.columns):\n",
        "            if row['type'] == 'last':\n",
        "                if col == 'type':\n",
        "                    worksheet.write(row_num, col_num, row[col],\n",
        "                                    last_blue_right)\n",
        "                elif col == entity_id:\n",
        "                    if link_templates_key is not None: # hyperlinks\n",
        "                        url = _link_templates[\n",
        "                            link_templates_key][\"record\"].format(\n",
        "                                entity_id = row[entity_id]\n",
        "                            )\n",
        "                        url = _excel_redirect(url) # excel nonsense\n",
        "                        if col_info[col][\"is_string\"]:\n",
        "                            worksheet.write_url(row_num, col_num, url,\n",
        "                                last_blue_left_link,\n",
        "                                string=str(row[entity_id]))\n",
        "                        else:\n",
        "                            worksheet.write_url(row_num, col_num, url,\n",
        "                                last_blue_right_link,\n",
        "                                string=str(row[entity_id]))\n",
        "                    else:\n",
        "                        if col_info[col][\"is_string\"]:\n",
        "                            worksheet.write(\n",
        "                                row_num, col_num, row[col], last_blue_left)\n",
        "                        else:\n",
        "                            worksheet.write(\n",
        "                                row_num, col_num, row[col], last_blue_right)\n",
        "                elif col == 'action': # blank and blue\n",
        "                    worksheet.write(row_num, col_num, '', last_blue_right)\n",
        "                elif col == 'score': # blank and blue\n",
        "                    worksheet.write(row_num, col_num, '', last_blue_right)\n",
        "                else:\n",
        "                    if col_info[col][\"is_string\"]:\n",
        "                        worksheet.write(row_num, col_num, row[col],\n",
        "                                        last_blue_left)\n",
        "                    elif col_info[col]['is_numeric']:\n",
        "                        if col_info[col]['max_decimals'] == 2:\n",
        "                            worksheet.write(row_num, col_num, row[col],\n",
        "                                            last_blue_right_dec2)\n",
        "                        else:\n",
        "                            worksheet.write(row_num, col_num, row[col],\n",
        "                                            last_blue_right)\n",
        "                    elif col_info[col]['is_date']:\n",
        "                        worksheet.write(row_num, col_num, row[col],\n",
        "                                        last_blue_right_date)\n",
        "                    else: # other?\n",
        "                        worksheet.write(row_num, col_num, row[col],\n",
        "                                        last_blue_right)\n",
        "            else: # prior\n",
        "                if col == 'type':\n",
        "                    worksheet.write(row_num, col_num, row[col], prior_right)\n",
        "                elif col == entity_id:\n",
        "                    if link_templates_key is not None: # hyperlinks\n",
        "                        url = _link_templates[\n",
        "                            link_templates_key][\"record\"].format(\n",
        "                                entity_id = row[entity_id]\n",
        "                            )\n",
        "                        url = _excel_redirect(url) # excel nonsense\n",
        "                        if col_info[col][\"is_string\"]:\n",
        "                            worksheet.write_url(row_num, col_num, url,\n",
        "                                prior_left_link, string=str(row[entity_id]))\n",
        "                        else:\n",
        "                            worksheet.write_url(row_num, col_num, url,\n",
        "                                prior_right_link, string=str(row[entity_id]))\n",
        "                    else:\n",
        "                        if col_info[col][\"is_string\"]:\n",
        "                            worksheet.write(\n",
        "                                row_num, col_num, row[col], prior_left)\n",
        "                        else:\n",
        "                            worksheet.write(\n",
        "                                row_num, col_num, row[col], prior_right)\n",
        "                elif col == 'action':\n",
        "                    if link_templates_key is True:\n",
        "                        url = _link_templates[\n",
        "                            link_templates_key][\"merge\"].format(\n",
        "                                entity_id = row[entity_id],\n",
        "                                last_id = row[\"last_id\"]\n",
        "                            )\n",
        "                        url = _excel_redirect(url) # excel nonsense\n",
        "                        worksheet.write_url(row_num, col_num, url,\n",
        "                            prior_left_link, string='merge')\n",
        "                elif col == 'score':\n",
        "                    worksheet.write(row_num, col_num, row[col],\n",
        "                                    prior_right_dec3)\n",
        "                else:\n",
        "                    if col_info[col][\"is_string\"]:\n",
        "                        #print(col)\n",
        "                        worksheet.write(row_num, col_num, row[col],\n",
        "                                        prior_left)\n",
        "                    elif col_info[col]['is_numeric']:\n",
        "                        if col_info[col]['max_decimals'] == 2:\n",
        "                            worksheet.write(row_num, col_num, row[col],\n",
        "                                            prior_right_dec2)\n",
        "                        else:\n",
        "                            worksheet.write(row_num, col_num, row[col],\n",
        "                                            prior_right)\n",
        "                    elif col_info[col]['is_date']:\n",
        "                        worksheet.write(row_num, col_num, row[col],\n",
        "                                        prior_right_date)\n",
        "                    else: # other?\n",
        "                        worksheet.write(row_num, col_num, row[col],\n",
        "                                        prior_right)\n",
        "\n",
        "    worksheet.freeze_panes(1, 4)\n",
        "    writer.close()\n",
        "    return xdf"
      ],
      "metadata": {
        "id": "xvgbS_wGXlMI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run()"
      ],
      "metadata": {
        "id": "xufeH01u9B5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run(\n",
        "    input_data, # file (xls,xlsx,csv) or dataframe\n",
        "    entity_id='entity_id',\n",
        "    search_columns = ['name', 'address', 'city', 'state'],\n",
        "    dedupe_passes = ['ALL'],  # ['state:2', 'name:3']\n",
        "    link_templates_key = None,\n",
        "    score_minimum = .67,\n",
        "    similar_column = None, # 'first_name' will exclude household matches\n",
        "    filter_column = None, #\n",
        "    filter_minimum = None, #\n",
        "    output_all = False, # True outputs all records, False is just the dups\n",
        "    _tuning = False,\n",
        "    _quiet = False,\n",
        "    _limit = None,\n",
        "):\n",
        "    global _last\n",
        "    _last = None\n",
        "    # suppress warning\n",
        "    import warnings\n",
        "    import dateutil\n",
        "    warnings.filterwarnings(\"ignore\",\n",
        "        category=dateutil.parser.UnknownTimezoneWarning)\n",
        "\n",
        "    if not _quiet: print(f\"{_time()} loading data\")\n",
        "    df = _load_data(input_data) # load file\n",
        "    if _limit is not None:\n",
        "        df = df[0:_limit].copy()\n",
        "\n",
        "    if not _quiet: print(f\"{_time()} finding duplicates\")\n",
        "    df = _find_duplicates(\n",
        "        df,\n",
        "        entity_id = entity_id,\n",
        "        search_columns = search_columns,\n",
        "        dedupe_passes = dedupe_passes,\n",
        "        score_minimum = .67,\n",
        "        similar_column = similar_column,\n",
        "    )\n",
        "\n",
        "    # refactor to do this in the above function\n",
        "    if filter_column is not None and filter_minimum is not None:\n",
        "        # filter on duplications involving filter_field at a filter_minimum\n",
        "        last_ids = list(\n",
        "            df[df[filter_column] >= filter_minimum]['last_id'].unique())\n",
        "        df = df[df['last_id'].isin(last_ids)]\n",
        "\n",
        "    if _tuning:\n",
        "        return df # run results for tuning and avoid excel output\n",
        "    else:\n",
        "        if not _quiet: print(f\"{_time()} outputting Excel\")\n",
        "        xdf = _output_excel(\n",
        "            df,\n",
        "            entity_id=entity_id,\n",
        "            search_columns=search_columns,\n",
        "            link_templates_key=link_templates_key,\n",
        "            output_all=output_all,\n",
        "        )\n",
        "        if not _quiet: print(f\"{_time()} results completed in output.xlsx\")\n",
        "        return xdf # formatted results for users - also in output.xlsx"
      ],
      "metadata": {
        "id": "JnHWuUYC9RF0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#module_end  todo: datamachine change to recognize this"
      ],
      "metadata": {
        "id": "WkGzCK-AIGfx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTS"
      ],
      "metadata": {
        "id": "7ZybEnjd9Lnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CRM Data (CSV)"
      ],
      "metadata": {
        "id": "OiberZ4D30rP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test run"
      ],
      "metadata": {
        "id": "Sy4MSfLca_TN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "if False:\n",
        "    df = run(\n",
        "        input_data = \"donors_try1.csv\", # exported from the CRM\n",
        "        entity_id = \"donor_id\", # unique id\n",
        "        search_columns = ['first_name', 'last_name', 'address', 'city', 'state'],\n",
        "        dedupe_passes = ['ALL'],\n",
        "        link_templates_key = \"donorperfect\",\n",
        "        similar_column = 'first_name',\n",
        "        filter_column = 'gift_total',\n",
        "        filter_minimum = 2_000,\n",
        "        output_all = False,\n",
        "        _tuning = False,\n",
        "        _quiet = False,\n",
        "    )\n",
        "    print('after')"
      ],
      "metadata": {
        "id": "bi80Ma1e3LBI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test  precision logic\n",
        "if False:\n",
        "    df.head() #.columns\n",
        "    df['email'] = df['email'].fillna('')\n",
        "    df2 = df[df['dup_flag'] == 2].copy() # last\n",
        "    df1 = df[df['dup_flag'] == 1].copy() # prior\n",
        "    cols = ['last_id','donor_id','first_name','last_name', 'address','city','state','email']\n",
        "    dfm = df2[cols].merge(df1[cols],on='last_id',suffixes=('2','1'))\n",
        "    dfm['category'] = ''\n",
        "    for (index,row) in dfm.iterrows():\n",
        "        if row.first_name2.lower().startswith('the'):\n",
        "            dfm.loc[index, 'category'] = 'OTHER'\n",
        "        else:\n",
        "            if ( row.address2.lower() == row.address1.lower()\n",
        "            and row.city2.lower() == row.city1.lower()\n",
        "            and row.state2.lower() == row.state1.lower()\n",
        "            ):\n",
        "                if ( _similar(row.last_name2.lower(), row.last_name1.lower())\n",
        "                and _similar(row.first_name2.lower(), row.first_name1.lower())\n",
        "                ):\n",
        "                    dfm.loc[index, 'category'] = 'single'\n",
        "                else:\n",
        "                    dfm.loc[index, 'category'] = 'household'\n",
        "            else:\n",
        "                dfm.loc[index, 'category'] = 'OTHER'\n",
        "    dfm"
      ],
      "metadata": {
        "id": "pVPd7ITFUnvp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _get_metrics()"
      ],
      "metadata": {
        "id": "IASt2js9QcNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "def _get_metrics(df, _quiet=False):\n",
        "    import pandas as pd\n",
        "    audit_df = df\n",
        "    audit_df['dup_flag'] = audit_df['dup_flag'].fillna(0)\n",
        "    audit_df['label'] = audit_df['label'].fillna(0)\n",
        "    # First, let's define what constitutes a true positive, false positive, and false negative in your context\n",
        "    # True Positive (TP): dup_flag is 1 or 2 AND label is 1 or 2\n",
        "    # False Positive (FP): dup_flag is 1 or 2 BUT label is NOT 1 or 2\n",
        "    # False Negative (FN): dup_flag is NOT 1 or 2 BUT label is 1 or 2\n",
        "    total = audit_df.shape[0]\n",
        "    labeled = audit_df[audit_df['label'].isin([1, 2])].shape[0]\n",
        "    found = audit_df[audit_df['dup_flag'].isin([1, 2])].shape[0]\n",
        "    TP = audit_df[(audit_df['dup_flag'].isin([1, 2])) & (audit_df['label'].isin([1, 2]))].shape[0]\n",
        "    FP = audit_df[(audit_df['dup_flag'].isin([1, 2])) & (~audit_df['label'].isin([1, 2]))].shape[0]\n",
        "    FN = audit_df[(~audit_df['dup_flag'].isin([1, 2])) & (audit_df['label'].isin([1, 2]))].shape[0]\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    if not _quiet:\n",
        "        print(f'total: {total}, labeled: {labeled}, found {found}')\n",
        "        print(f\"Precision: {precision*100:.2f}% : Of those found, the percent that were labeled\")\n",
        "        print(f\"   Recall: {recall*100:.2f}% : Of those labeled, the percent that were found.\")\n",
        "    return (total, labeled, found, precision, recall)\n",
        "#_get_metrics(df)"
      ],
      "metadata": {
        "id": "Y5qmDhEodGEa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _tuning()"
      ],
      "metadata": {
        "id": "bSROihVfQhkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test  tuning parameters loop TODO:  map parameters through run\n",
        "#def _tuning():\n",
        "if False:\n",
        "    import pandas as pd\n",
        "    from io import StringIO\n",
        "    import ast\n",
        "    import time\n",
        "    zdf = pd.read_csv(StringIO( \"\"\"\\\n",
        "    secs|precision|recall|file|passes\n",
        "    0|0.0|0.0|labels2.csv|['first_name:2']\n",
        "    0|0.0|0.0|labels2.csv|['last_name:2']\n",
        "    0|0.0|0.0|labels2.csv|['last_name:2']\n",
        "    0|0.0|0.0|labels2.csv|['state:2']\n",
        "    0|0.0|0.0|labels2.csv|['ALL']\n",
        "    0|0.0|0.0|labels2.csv|['state:2','ALL']\n",
        "    \"\"\"), delimiter='|',\n",
        "    dtype={})\n",
        "    for index, row in zdf.iterrows():\n",
        "        start_time = round(time.time())\n",
        "\n",
        "        print(f'test:{index} file:{row[\"file\"]} passes:{row[\"passes\"]}')\n",
        "        passes = ast.literal_eval(row['passes'])\n",
        "        df = run(\n",
        "            input_data = row['file'], # exported from the CRM\n",
        "            entity_id = \"donor_id\", # unique id\n",
        "            search_columns = ['first_name', 'last_name', 'address', 'city', 'state'],\n",
        "            dedupe_passes = passes,\n",
        "            link_templates = \"donorperfect\",\n",
        "            output_all = True, # testing\n",
        "            _tuning = True,\n",
        "            _quiet = True,\n",
        "        )\n",
        "        (total, labeled, found, precision, recall) = _get_metrics(df, _quiet=True)\n",
        "        zdf.at[index, 'precision'] = precision\n",
        "        zdf.at[index, 'recall'] = recall\n",
        "        zdf.at[index, 'secs'] = round(time.time()) - start_time\n",
        "    zdf"
      ],
      "metadata": {
        "id": "RX0x6gDMAriI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test  merge forward labels\n",
        "if False:\n",
        "    import pandas as pd\n",
        "    df1 = pd.read_csv('donors2.csv')\n",
        "    df2 = pd.read_csv(\"labels2.csv\")\n",
        "    result_df = pd.merge(df1, df2[['donor_id', 'label']], on='donor_id', how='left')\n",
        "    result_df.to_csv(\"labels3.csv\")"
      ],
      "metadata": {
        "id": "-5G6YX99bVW1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "# diagnose:\n",
        "if False:\n",
        "    print(f'dups found:', df.shape[0])\n",
        "    print(f'ALLS found:', df[df['pass'] == 'ALL'].shape[0])\n",
        "    display(df[\n",
        "        ['type','donor_id','last_id','last_name','city','pass']]\n",
        "            .sort_values(['pass','last_id','donor_id'], ascending=[True,False,False]))"
      ],
      "metadata": {
        "id": "OIv-C8yb4h6o"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "if False:\n",
        "    print(df[crit].shape)\n",
        "    df[crit][['donor_id','last_id','dup_flag','score',\n",
        "            'first_name', 'last_name', 'address', 'city', 'state','zip','gift_total','last_contrib_date']].sort_values(\n",
        "                ['last_id','dup_flag'], ascending=[False, False])"
      ],
      "metadata": {
        "id": "JyAWMiAZZQWw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "if False:\n",
        "    import pandas as pd\n",
        "    df['prefix'] = df['last_name'].str[:2]\n",
        "    groups_with_diff_city_prefix = df.groupby('last_id').filter(lambda x: x['prefix'].nunique() > 1)\n",
        "    result = groups_with_diff_city_prefix.sort_values(by=['last_id', 'prefix'])\n",
        "    display(result[['donor_id','last_id','dup_flag','score',\n",
        "            'first_name', 'last_name', 'address', 'city', 'state','zip','gift_total','last_contrib_date']]\n",
        "            .sort_values(['last_id','dup_flag'], ascending=False))"
      ],
      "metadata": {
        "id": "J3QgABOm86Wx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Data"
      ],
      "metadata": {
        "id": "urQeDlNKXas3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _first_name_variations"
      ],
      "metadata": {
        "id": "BgmwhJFF6mhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "_first_name_variations = {\n",
        "    'Dave': 'David',\n",
        "    'Joe': 'Joseph',\n",
        "    'Elizabeth': 'Betty',\n",
        "    'Bill': 'William',\n",
        "    'Bob': 'Robert',\n",
        "    'Tom': 'Thomas',\n",
        "    'Dick': 'Richard',\n",
        "    'Jim': 'James',\n",
        "    'Jenny': 'Jennifer',\n",
        "    'Kathy': 'Katherine',\n",
        "    'Sue': 'Susan',\n",
        "    'Maggie': 'Margaret',\n",
        "    'Harry': 'Harold',\n",
        "    'Ron': 'Ronald',\n",
        "    'Nick': 'Nicholas',\n",
        "    'Patty': 'Patricia',\n",
        "    'Chris': 'Christopher',\n",
        "    'Mike': 'Michael',\n",
        "    'Steve': 'Steven',\n",
        "    'Andy': 'Andrew',\n",
        "    'Matty': 'Matthew',\n",
        "    'Sam': 'Samuel',\n",
        "    'Tony': 'Anthony',\n",
        "}"
      ],
      "metadata": {
        "id": "zVPgVYRmw-v_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _first_name_variation()"
      ],
      "metadata": {
        "id": "ihgJUudWDTIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "def _first_name_variation(full_name, chance=40):\n",
        "    import random\n",
        "    name_parts = full_name.split()\n",
        "    first_name = name_parts[0]\n",
        "    if random.random() < (chance / 100.0):\n",
        "        if first_name in _first_name_variations:\n",
        "            first_name = _first_name_variations[first_name]\n",
        "        else:\n",
        "            for key, value in _first_name_variations.items():\n",
        "                if first_name == value:\n",
        "                    first_name = key\n",
        "                    break\n",
        "    return ' '.join([first_name] + name_parts[1:])\n",
        "\n",
        "#print(_first_name_variation(\"Joe Satriani\",chance=100))\n",
        "#print(_first_name_variation(\"David Bowie\",chance=100))"
      ],
      "metadata": {
        "id": "laWrKB3oyKdU"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _last_name_transposition()"
      ],
      "metadata": {
        "id": "aHeL4BiU6soR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "def _last_name_transposition(full_name, chance=1):\n",
        "    import random\n",
        "    name_parts = full_name.split()\n",
        "    if len(name_parts) > 1 and random.random() < (chance / 100.0):\n",
        "        last_name = name_parts[-1]\n",
        "        if len(last_name) > 4:\n",
        "            i = random.randint(1, len(last_name) - 2)\n",
        "            last_name_chars = list(last_name)\n",
        "            last_name_chars[i], last_name_chars[i+1] = \\\n",
        "              last_name_chars[i+1], last_name_chars[i]\n",
        "            name_parts[-1] = ''.join(last_name_chars)\n",
        "    return ' '.join(name_parts)\n",
        "#print(_last_name_transposition(\"Jon Anderson\", chance=100))"
      ],
      "metadata": {
        "id": "AO5lXRJN2KGC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _generate_test_data()"
      ],
      "metadata": {
        "id": "kWixweujGFWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "!pip install mimesis # used for generating test data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUFCr1OM_91h",
        "outputId": "817aea62-d9c9-4507-80d7-d8cf8eff36d0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mimesis\n",
            "  Downloading mimesis-14.0.0-py3-none-any.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mimesis\n",
            "Successfully installed mimesis-14.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "def _generate_test_data(n):\n",
        "\n",
        "    from mimesis import Person, Address, Finance\n",
        "    import pandas as pd\n",
        "    import random\n",
        "\n",
        "    data = []\n",
        "    entity_id = 0\n",
        "\n",
        "    person = Person('en')\n",
        "    address_gen = Address('en')\n",
        "    finance = Finance('en')\n",
        "\n",
        "    for _ in range(n):\n",
        "        entity_id += 1\n",
        "        is_person = True\n",
        "        name = person.full_name()\n",
        "        if random.random() < (10.0 / 100.0):\n",
        "            is_person = False\n",
        "            name = finance.company()\n",
        "        street_address = address_gen.address()\n",
        "        city = address_gen.city()\n",
        "        state = address_gen.state(abbr=True)\n",
        "        label = 0  # not a duplicate\n",
        "        data.append([entity_id, name, street_address, city, state, label])\n",
        "        if random.random() < (1.0 / 100.0): # exact duplicate\n",
        "            entity_id += 1\n",
        "            label = 1\n",
        "            data[-1][-1] = label # update the prior label\n",
        "            data.append([entity_id, name, street_address, city, state, label])\n",
        "        if is_person:\n",
        "            dup_name = _first_name_variation(name, chance=30)\n",
        "            dup_name = _last_name_transposition(dup_name, chance=1)\n",
        "            if name != dup_name: # fuzzy duplicate\n",
        "                entity_id += 1\n",
        "                label = 1\n",
        "                data[-1][-1] = label # update the prior label\n",
        "                data.append([\n",
        "                    entity_id, dup_name, street_address, city, state, label])\n",
        "    random.shuffle(data) # mix them up!\n",
        "    df = pd.DataFrame(data,\n",
        "        columns=['entity_id', 'name', 'address', 'city', 'state', 'label'])\n",
        "    df['entity_id'] = df.index + 1 # renumber to shuffled order\n",
        "    df.to_csv('test.csv', index=False) # test data for passing to _load_data()\n",
        "\n",
        "    return df\n",
        "#_generate_test_data(1000)"
      ],
      "metadata": {
        "id": "TKpjq_Q_nKzZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "if False:\n",
        "    test_df = _generate_test_data(10000)"
      ],
      "metadata": {
        "id": "vXFcHUrBSsCd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "if False:\n",
        "    \"\"\"\n",
        "    class m: pass\n",
        "    test_df = _generate_test_data(1000)\n",
        "    glabel = test_df['label'] > 0\n",
        "    print('test labels:', test_df[glabel].shape[0])\n",
        "    df = _tfidf_neighbors(test_df, threshold=.75)\n",
        "\n",
        "    #print('tfidf labels:', df[df['label'] > 0].shape[0])\n",
        "    #print('tfidf labels found:', df[(df['label'] > 0) & (df['dup_flag'] > 0)].shape[0])\n",
        "    #print('tfidf found:', df[(df['dup_flag'] > 0)].shape[0])\n",
        "    \"\"\"\n",
        "    crit = (df['dup_flag'] > 0) # | (df['label'] > 0)\n",
        "    print(df[crit].shape)\n",
        "    df[crit][['entity_id','last_id','dup_flag','score','label',\n",
        "            'name','address','city','state','combined']].sort_values(\n",
        "                ['last_id','dup_flag'], ascending=[False, False])"
      ],
      "metadata": {
        "id": "xE-rkFRbZOhY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "if False:\n",
        "    # prep generated data\n",
        "    compare = ['name', 'address', 'city', 'state']\n",
        "    reference = ['label']\n",
        "    columns_order = ['entity_id', 'last_id', 'dup_flag', 'score'] + compare + reference\n",
        "    xdf = df[crit][columns_order].sort_values(\n",
        "                ['last_id','dup_flag'], ascending=[False, False])\n",
        "    xdf = xdf[0:500].reset_index(drop=True)\n",
        "    xdf.shape[0]"
      ],
      "metadata": {
        "id": "Mc7jngsZczqy"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "# ['first_name', 'last_name', 'address', 'city', 'state']\n",
        "# prep exported  data\n",
        "if False:\n",
        "    crm_flag = True\n",
        "    entity_id = 'donor_id'\n",
        "\n",
        "    compare = ['first_name', 'last_name', 'address', 'city', 'state']\n",
        "    reference = ['gift_total', 'created_by', 'last_contrib_date', 'last_contrib_amt', 'zip']\n",
        "    columns_order = initial + compare + reference\n",
        "    xdf = df[crit][columns_order].sort_values(['last_id','dup_flag'], ascending=[False, False])\n",
        "    xdf = xdf[0:500].reset_index(drop=True)\n",
        "    xdf.rename(columns={'dup_flag': 'type'}, inplace=True)\n",
        "    xdf['type'] = xdf['type'].map({2: 'last', 1: 'prior'}).astype(str)\n",
        "    if crm_flag is True:\n",
        "        xdf.insert(loc=3, column='action', value='')\n",
        "    output_columns = xdf.columns.tolist() # direct mapping to output\n",
        "    print(output_columns)\n",
        "    xdf #.shape[0]"
      ],
      "metadata": {
        "id": "nbHVYiZfade1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyses"
      ],
      "metadata": {
        "id": "dMZCwsEilNau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _rows_vs_time_plot()"
      ],
      "metadata": {
        "id": "6naBwTMap3nL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "def _rows_vs_time_plot():\n",
        "    import pandas as pd\n",
        "    import plotly.express as px\n",
        "    import numpy as np\n",
        "    cdf = pd.DataFrame()\n",
        "    cdf['rows'] = list(range(10_000, 1_010_000, 10_000))\n",
        "    cdf['compares'] = cdf['rows'] * (cdf['rows'] - 1) / 2\n",
        "    cdf['time'] = cdf['compares'] / 21_766_643  # seconds\n",
        "    cdf['time_delta'] = pd.to_timedelta(cdf['time'], unit='s')\n",
        "    cdf['time_seconds'] = cdf['time_delta'].dt.total_seconds()\n",
        "    max_hours = cdf['time_delta'].dt.total_seconds().max() / 3600\n",
        "    tick_interval_minutes = 15\n",
        "    tick_interval = pd.Timedelta(minutes=tick_interval_minutes)\n",
        "    tick_vals = [i*tick_interval for i in range(int(max_hours*6) + 1)]\n",
        "    tick_vals_seconds = [tick.total_seconds() for tick in tick_vals]\n",
        "    tick_labels = [(tick.total_seconds() // 3600,\n",
        "    (tick.total_seconds() % 3600) // 60) for tick in tick_vals]\n",
        "    tick_text = [f\"{int(h):02d}:{int(m):02d}\" for h, m in tick_labels]\n",
        "    fig = px.line(cdf, x='rows', y='time_seconds', labels={'time_delta': 'Time (hh:mm)'})\n",
        "    fig.update_traces(textposition='top center')\n",
        "    fig.update_yaxes(tickvals=tick_vals_seconds, ticktext=tick_text)\n",
        "    fig.update_xaxes(tickvals=list(range(50_000, 1_010_000, 50_000)), tickangle=-90)\n",
        "    fig.update_layout(yaxis_title=\"Time (hh:mm)\", xaxis_title=\"Number of Rows\")\n",
        "    fig.show()\n",
        "#_rows_vs_time_plot()"
      ],
      "metadata": {
        "id": "dJcfSHf6RYhB"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}